{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Loading dataset"
      ],
      "metadata": {
        "id": "We1M0nekLpKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to dataset file on Google Drive\n",
        "folder_path2014 = \"/content/drive/MyDrive/SEM 06/6SP/Raw Operational Data from Enterprise Application - KAGGLE.zip (Unzipped Files)/2014\"\n",
        "folder_path2015 = \"/content/drive/MyDrive/SEM 06/6SP/Raw Operational Data from Enterprise Application - KAGGLE.zip (Unzipped Files)/2015\"\n",
        "\n",
        "folders = [folder_path2014, folder_path2015]\n",
        "\n",
        "# Log file list\n",
        "log_files = []\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/SEM 06/6SP/all_log_files.txt\"\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(file_path, 'w') as file:\n",
        "  for folder in folders:\n",
        "    for dirname, _, filenames in os.walk(folder):\n",
        "        for filename in filenames:\n",
        "            # print(os.path.join(dirname, filename))\n",
        "            # Write content to the file\n",
        "            file.write(os.path.join(dirname, filename))\n",
        "            file.write(\"\\n\")\n",
        "\n",
        "\n",
        "print(\"All file names saved to all_log_files.txt \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sOYTZMCLqmv",
        "outputId": "bc51a479-4422-41b0-ad58-b71e81dbd792"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "All file names saved to all_log_files.txt \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Set the range for random numbers\n",
        "start_range = 1\n",
        "end_range = 18337\n",
        "num_samples = 10\n",
        "\n",
        "# Generate a list of random numbers\n",
        "random_numbers = random.sample(range(start_range, end_range + 1), num_samples)\n",
        "\n",
        "# Print the list of random numbers\n",
        "print(random_numbers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjwz03zyXRv9",
        "outputId": "6725123e-bdbc-412f-a8a7-1d137dec4c64"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16030, 2498, 228, 1358, 12898, 11977, 3515, 9022, 11331, 9424]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating dataframes from log files\n"
      ],
      "metadata": {
        "id": "ItD4kdPpT8lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "# df = pd.read_csv(file_path)\n",
        "# print(df)\n",
        "\n",
        "# List of dataframes\n",
        "dfs = []\n",
        "\n",
        "\n",
        "# file_path = \"/content/drive/MyDrive/SEM 06/6SP/all_log_files.txt\"\n",
        "\n",
        "i = 0 # For line number\n",
        "\n",
        "# Open the file in read mode\n",
        "with open(file_path, 'r') as file:\n",
        "    # Read the file line by line\n",
        "    for line in file:\n",
        "      i += 1\n",
        "      if i > max(random_numbers):\n",
        "        break\n",
        "      if i in random_numbers:\n",
        "        dfs.append( pd.read_csv(line.strip(),low_memory=False))\n",
        "\n",
        "\n",
        "print(len(dfs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFG14kpnV9d9",
        "outputId": "252d5446-e355-4c35-e44c-bfdbd27703e8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dropping empty log file dataframes"
      ],
      "metadata": {
        "id": "ZqWKVLgvYhtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Initial no. of dataframes = \",len(dfs))\n",
        "empty_file_count = 0\n",
        "i = -1\n",
        "for df in dfs:\n",
        "  i +=1\n",
        "  if len(df) == 0:\n",
        "    empty_file_count +=1\n",
        "    print(\"Empty file found! Total = \",empty_file_count)\n",
        "    del dfs[i]\n",
        "\n",
        "\n",
        "print(\"Final no. of dataframes = \",len(dfs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PL6ncfGYqzY",
        "outputId": "b9c8fc47-3af0-42ba-94fe-cba412c9dcb4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial no. of dataframes =  10\n",
            "Empty file found! Total =  1\n",
            "Final no. of dataframes =  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Viewing some df from dfs"
      ],
      "metadata": {
        "id": "wjVyxvRPc3Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(dfs[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iDiXhdKc2ph",
        "outputId": "9c8d787e-edc6-4438-e6f5-74cb3f8d385a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating log file clusters based on column names"
      ],
      "metadata": {
        "id": "lX_RkwkMd3WX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# # List of log file paths\n",
        "# log_files = ['log_file1.csv', 'log_file2.csv', 'log_file3.csv', ...]\n",
        "\n",
        "# Load log files into separate DataFrames\n",
        "dataframes = dfs\n",
        "# for file in log_files:\n",
        "#     df = pd.read_csv(file)\n",
        "#     dataframes.append(df)\n",
        "\n",
        "# Extract column names from each DataFrame\n",
        "column_names = []\n",
        "for df in dataframes:\n",
        "    column_names.append(df.columns.tolist())\n",
        "\n",
        "# print(column_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "zG_bka7cd_KD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/SEM 06/6SP/columns.txt\"\n",
        "# Open the file in write mode\n",
        "with open(file_path, 'w') as file:\n",
        "  for cols in column_names:\n",
        "    for col in cols:\n",
        "      file.write(col)\n",
        "      file.write('\\n')\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "\n",
        "# Create feature matrix\n",
        "vectorizer = CountVectorizer(binary=True)\n",
        "X = vectorizer.fit_transform(lines)\n",
        "\n",
        "# Apply dimensionality reduction\n",
        "pca = PCA(n_components=2)  # You can adjust the number of components\n",
        "X_reduced = pca.fit_transform(X.toarray())\n",
        "\n",
        "# Perform clustering with DBSCAN\n",
        "scaler = StandardScaler()\n",
        "X_reduced_scaled = scaler.fit_transform(X_reduced)\n",
        "\n",
        "# dbscan = DBSCAN(eps=0.5, min_samples=2)  # Adjust the eps and min_samples values <----------------------------------\n",
        "# clusters = dbscan.fit_predict(X_reduced_scaled)\n",
        "\n",
        "\n",
        "# # Assign log files to clusters\n",
        "# log_files_clusters = {}\n",
        "# for i, file in enumerate(log_files):\n",
        "#     log_files_clusters[file] = clusters[i]\n",
        "\n",
        "# # Print the log file and its corresponding cluster\n",
        "# for file, cluster in log_files_clusters.items():\n",
        "#     print(f\"Log file: {file}, Cluster: {cluster}\")"
      ],
      "metadata": {
        "id": "MwwF40ievSl5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbscan = DBSCAN(eps=0.5, min_samples=2)  # Adjust the eps and min_samples values <----------------------------------\n",
        "clusters = dbscan.fit_predict(X_reduced_scaled)\n",
        "\n",
        "print(clusters)\n",
        "\n",
        "\n",
        "# Assign log files to clusters\n",
        "log_files_clusters = {}\n",
        "for i, file in enumerate(log_files):\n",
        "    log_files_clusters[file] = clusters[i]\n",
        "\n",
        "# Print the log file and its corresponding cluster\n",
        "for file, cluster in log_files_clusters.items():\n",
        "    print(f\"Log file: {file}, Cluster: {cluster}\")\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "print(\"Done!!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y21tEVBGzPbb",
        "outputId": "ec3cd314-7ec8-4569-f859-270f5549c572"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 ... 0 0 0]\n",
            "Done!!!\n"
          ]
        }
      ]
    }
  ]
}